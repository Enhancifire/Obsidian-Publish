{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Faiz Saiyad's Obsidian Notes","text":""},{"location":"#college","title":"College","text":"<ul> <li>Explainable Artificial Intelligence for Engineering</li> </ul>"},{"location":"Categorical%20Boosting%20Models/","title":"Categorical Boosting Models","text":""},{"location":"Categorical%20Boosting%20Models/#what-are-categorical-boosting-models","title":"What are Categorical Boosting Models?","text":"<p>Categorical Boosting Models, also called Gradient Boosting Machines (GBM) are a type of Ensemble Learning Method specifically designed to handle categorical or discrete features in input data. </p>"},{"location":"Categorical%20Boosting%20Models/#algorithms-of-categorical-boosting-model","title":"Algorithms of Categorical Boosting Model","text":"<ol> <li>One Hot Encoding    The first step is converting the categorical data into numerical format. This is done by converting n unique categories in the features into binary form, by turning them into whether the category is present or not.    Ex. In a feature of disease symptoms, we can convert the various symptoms into binary format by converting them into columns and assigning them values of 0 if not present and 1 when present</li> <li>Gradient Boosting for Categorical Variables    Categorical boosting models employ diff. strategies to handle categorical splits in the data</li> <li>Categorical Splitting Strategies</li> <li>Ordinal Encoding       Assigning a numerical value to each category based on order. This allows the algo to treat categorical variables as continuous data.</li> <li>One Versus All Encoding       Create binary features by encoding each category as a separate binary feature</li> <li>Frequency Encoding       Replace each category with its frequency in the training dataset.</li> <li>Tree Construction    The algorithm then generates a series of decision trees based on the features. Each tree is made using Gradient Boosting technique. It takes into account both the numerical and categorical data.</li> <li>Model Ensemble    The final prediction is obtained by combining the predictions of the generated trees, generally done through weighted voting.</li> </ol>"},{"location":"Contrastive%20Explanations%20and%20LRP%20for%20Machine%20Learning/","title":"Contrastive Explanations and LRP for Machine Learning","text":"<p>Unit 6 of Explainable Artificial Intelligence for Engineering</p>"},{"location":"Contrastive%20Explanations%20and%20LRP%20for%20Machine%20Learning/#contents","title":"Contents:","text":"<ol> <li>Contrastive Explanations</li> <li>Layer-wise Relevance Propagation</li> </ol>"},{"location":"Contrastive%20Explanations/","title":"Contrastive Explanations","text":""},{"location":"Contrastive%20Explanations/#what-is-contrastive-explanations-for-ml","title":"What is Contrastive Explanations for ML?","text":"<p>Contrastive Explanations are explanations that compare and highlight the differences between two or more scenarios. They aim to explain why a certain prediction was made by emphasizing the contrasting factors or features between different scenarios.</p> <p>They provide insights into how changing specific attributes or conditions leads to different outcomes, similar to What If Tool.</p> <p>Let's take an example. We have a robot that sorts apples and oranges by using a camera depending on their color and shape. Sometimes, it misclassifies apples as oranges and vice versa.</p> <p>To explain the robots decision and highlight the differences, a CE would show two images side by side, one being a correctly identified apple and other being mistakenly identified as orange. The explanation would point out the specific color and shape differences between the two images that caused the robot to make the wrong prediction. By demonstrating the contrasting factors that influenced the decision, it helps us identify potential areas of improvement or sources of error.</p> <p>Various approaches may be used to generate CE, such as feature importance analysis, similarity metrics, highlighting feature variations between data instances, etc.</p> <p>There are two concepts associated with contrastive explanations: 1. Pertinent Positives (PP): Pertinent positives refer to the features or factors that are present in instances that receive a positive prediction or outcome but are absent or different in instances that receive a negative prediction or outcome. These features highlight the factors that contribute to a positive outcome or prediction. 2. Pertinent Negatives (PN): Pertinent negatives, on the other hand, refer to the features or factors that are absent or different in instances that receive a positive prediction or outcome compared to instances that receive a negative prediction or outcome. These features highlight the factors that contribute to a negative outcome or prediction.</p>"},{"location":"Contrastive%20Explanations/#cem-using-alibi","title":"CEM using Alibi","text":"<p>Contrastive Explanations for Models (CEMs) can be implemented using Alibi, a Python Library.</p>"},{"location":"Contrastive%20Explanations/#use-cases-of-contrastive-explanations","title":"Use Cases of Contrastive Explanations","text":""},{"location":"Contrastive%20Explanations/#comparison-of-original-vs-autoencoder-generated-image","title":"Comparison of Original vs Autoencoder Generated Image","text":"<p>Autoencoder is a ML model that can compress and decompress images. When comparing to an original image, the autoencoder attempts to decompress/reconstruct the original image from a compressed representation.  By comparing the differences between the two images, we can assess the quality of the decompression and how well the autoencoder captures the essential features of the original image.</p>"},{"location":"Contrastive%20Explanations/#cem-for-tabular-data-explanations","title":"CEM for Tabular Data Explanations","text":"<p>CEMs can be applied to tabular data to generate Counterfactual Explanations for individual instances. By altering the input values of a specific data instance, CEM can find alternate values that would lead to a different prediction. This helps provide explanations for individual predictions in tabular data sections.</p>"},{"location":"Counterfactual%20Explanations%20for%20XAI%20Models/","title":"Counterfactual Explanations for XAI Models","text":"<p>Unit 5 of Explainable Artificial Intelligence for Engineering</p>"},{"location":"Counterfactual%20Explanations%20for%20XAI%20Models/#contents","title":"Contents","text":"<ol> <li>What If Tool</li> <li>Counterfactual Explanations</li> </ol>"},{"location":"Counterfactual%20Explanations/","title":"Counterfactual Explanations","text":""},{"location":"Counterfactual%20Explanations/#what-are-counterfactual-explanations","title":"What are Counterfactual Explanations?","text":"<p>Counterfactual Explanations (CFEs) are causal relationships in the prediction process of ML Models. It is a process of creating hypothetical scenarios and generating predictions under those scenarios.</p> <p>Let's take an example. Suppose a model was supposed to predict whether a person will get a cookie or not. It would take a look at various factors like how many chores that person did, how much did they play, etc.</p> <p>Now, let's assume the model did not give someone a cookie. You might wonder what would have happened if that person might have done more chores and played less. CFEs are a way to understand this. We would create a pretend scenario where the person did more work or played less, and then we would see whether the model would give them a cookie or not. In this way, we can create scenarios to figure out which features are important to get the desired predictions and what changes would make a difference.</p> <p>By generating counterfactual explanations, XAI techniques attempt to provide a better understanding of the decision-making process of machine learning models.</p> <p>Counterfactual Explanations can be generated for both Regression and Classification tasks.</p>"},{"location":"Counterfactual%20Explanations/#cfe-using-alibi-library","title":"CFE using Alibi Library","text":"<p>CFEs can be generated using Alibi library in Python. Alibi library requires a trained model and a new dataset for which predictions need to be generated.</p> <p>Read From Page 298 of CMB Practical Explainable AI using Python for Example</p>"},{"location":"Ensemble%20Bagging/","title":"Ensemble: Bagging Technique","text":"<p>In the Bagging technique, the output of multiple models is taken and combined as average to generate the final result. Random forest is a good example of bagging algorithm. It grows multiple trees and combines the average of their output to generate the result</p>","tags":["XAI"]},{"location":"Ensemble%20Boosting/","title":"Ensemble: Boosting Technique","text":"<p>In Boosting, multiple weak models are combined to generate a strong predictive model. In boosting, we train multiple weak models, and in subsequent steps focus on correcting the mistakes made by the previous models. This is done till there is no noticeable change in accuracy of the final result.</p>"},{"location":"Ensemble%20Boosting/#steps-of-boosting","title":"Steps of Boosting","text":"<ol> <li>Weak Model Training:    Select a weak learning algorithm as base model.</li> <li>Training:    Train the model on training data. In subsequent runs, the model should focus more on the data points that were misclassified. </li> <li>Instance Weighting    Boosting assigns weights to the various models to focus on more difficult examples. First we start with equal weights, and as training progresses we assign higher weights to misclassified examples to have subsequent models focus more on them.</li> <li>Model Combination    After each weak model is trained, the predictions are combined with the predictions of the previous models. The models are all assigned weights according to their performance. Higher weights are assigned to more accurate models.</li> <li>Iterative Process    Steps 1-4 are repeated for some number of cycles or till no noticeable change in accuracy is seen.</li> <li>Final Prediction    The final prediction is acquired by combining the individual predictions of the weak models. This is done by using weighted voting where each model is assigned a weight based on its performance and importance.</li> </ol>"},{"location":"Ensemble%20Boosting/#two-types-of-boosting","title":"Two Types of Boosting","text":"<p>Adaptive Boosting (AdaBoost): - Trains models by assigning weights to examples based on accuracy of weak models - Has the subsequent models focus more no misclassified examples</p> <p>Gradient Boosting: - Trains the models on residuals of the previous models - Optimises the model by minimising loss function</p>"},{"location":"Ensemble%20Models/","title":"Ensemble Models","text":"<p>Ensemble Models: Complex Models that are made up of multiple smaller, less accurate models. Three types of Ensemble Models: 1. Bagging 2. Boosting 3. Stacking</p>"},{"location":"Ensemble%20Stacking/","title":"Ensemble: Stacking Technique","text":"<p>In Stacking, a Meta Model is trained on the outputs of various models. This method is used to find the best way to combine the outputs of various ML models into one accurate result.</p>"},{"location":"Ensemble%20Stacking/#steps-in-stacking","title":"Steps in Stacking","text":"<ol> <li>Base Model Training    In this step, many models are trained on the base dataset. These may be based on any algorithm like regression, random forest, etc.</li> <li>Creating Stacking Dataset    In this step, the predictions of the various models are used to create a stacking dataset that contains the predictions of each model per example.</li> <li>Meta Model Training    In this step, a model is trained on the stacking dataset, generally using the base dataset as label or target. Any algorithm can be used for the final meta model. This meta model results in accurate predictions</li> <li>Prediction    In this step, the input is given to the base models to generate predictions. The predictions made by the base models are fed into the meta model and it's prediction is treated as the final result</li> </ol>"},{"location":"Explainability%20for%20Ensemble%20Models/","title":"Explainability for Ensemble Models","text":""},{"location":"Explainability%20for%20Ensemble%20Models/#what-are-ensemble-models","title":"What are Ensemble Models?","text":"<p>Ensemble Models are Complex Models built using a combination of multiple small/weak models. They boast superior accuracy as compared to their base models.</p>"},{"location":"Explainability%20for%20Ensemble%20Models/#using-shap-for-ensemble-models","title":"Using SHAP for Ensemble Models","text":"<p>Explainability for Ensemble models is very important because we need to be able to understand about how the models are being combined.</p> <p>We can use SHAP for understanding the output of an ensemble model.</p>"},{"location":"Explainability%20for%20Ensemble%20Models/#steps-for-applying-shap-to-ensemble-models","title":"Steps for Applying SHAP to Ensemble Models","text":"<ol> <li>Understanding the Ensemble Model    Begin by understanding the model. Explore which models are being used as the base, as well as the type of ensemble method used</li> <li>Individual Model Predictions    To use SHAP, we need to obtain the individual results from each of the base models. This involves running the data through the individual models and recording the predictions</li> <li>Apply SHAP to the Individual Prediction    Apply SHAP to the predictions of the individual models. Use the appropriate SHAP algorithm for the model.</li> <li>Combine SHAP Values    Combine the SHAP values for the individual models based on the ensemble technique used. </li> <li>For Bagging, take the average of the SHAP values</li> <li>For Boosting, take the weighted average depending on the weights assigned to the individual models in the boosting model</li> <li>For Stacking, you can use the meta-models coefficients to generate the final SHAP value</li> <li>Interpret the SHAP Values    Interpret the final SHAP values to understand the feature importance and various other stuff that you do using SHAP</li> </ol>"},{"location":"Explainability%20for%20Ensemble%20Models/#using-shap-for-categorical-boosting-models","title":"Using SHAP for Categorical Boosting Models","text":"<ol> <li>Train the Categorical Boosting Models    Train the model on the training data.</li> <li>Convert Categorical Features    Since SHAP works on Numerical Features, we need to convert the categorical data into numerical form. We can use many techniques like One-Hot Encoding</li> <li>Calculate SHAP Values    Once we have converted the categorical data into numerical form, we can calculate the SHAP values for them. There exist specific implementations of SHAP for the various models, such as LightGBM SHAP for LightGBM, etc</li> <li>Interpret SHAP Values    We can now interpret the SHAP values to understand feature importance, etc</li> </ol>"},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/","title":"Explainable Artificial Intelligence for Engineering","text":"<p>Created at: [[2022-05-25]]</p>"},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#college-course","title":"College Course","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#units","title":"Units","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-i-introduction-to-explainable-artificial-intelligence","title":"Unit I: [[Introduction to Explainable Artificial Intelligence]]","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-ii-explainability-for-linear-models","title":"Unit II: [[Explainability for Linear Models]]","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-iii-explainability-for-non-linear-models","title":"Unit III: Explainability for Non Linear Models","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-iv-explainability-for-ensemble-models","title":"Unit IV: Explainability for Ensemble Models","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-v-counterfactual-explanations-for-xai-models","title":"Unit V: Counterfactual Explanations for XAI Models","text":""},{"location":"Explainable%20Artificial%20Intelligence%20for%20Engineering/#unit-vi-contrastive-explanations-and-lrp-for-machine-learning","title":"Unit VI: Contrastive Explanations and LRP for Machine Learning","text":""},{"location":"Internet%20of%20Things/","title":"Internet of Things","text":"<p>Created at: [[2022-05-25]]</p>"},{"location":"Internet%20of%20Things/#college-course","title":"College Course","text":""},{"location":"Internet%20of%20Things/#units","title":"Units","text":""},{"location":"Internet%20of%20Things/#unit-i-introduction-to-iot","title":"Unit I: Introduction to IoT","text":""},{"location":"Internet%20of%20Things/#unit-ii-protocols-for-iot","title":"Unit II: [[Protocols for IoT]]","text":""},{"location":"Internet%20of%20Things/#unit-iii-iot-and-m2m","title":"Unit III: IoT and M2M","text":""},{"location":"Internet%20of%20Things/#unit-iv-security-in-iot","title":"Unit IV: [[Security in IoT]]","text":""},{"location":"Internet%20of%20Things/#unit-v-cloud-computing-and-fog-computing","title":"Unit V: [[Cloud Computing]] and Fog Computing","text":""},{"location":"Internet%20of%20Things/#unit-vi-industrial-iot","title":"Unit VI: Industrial IoT","text":""},{"location":"Layer-wise%20Relevance%20Propagation/","title":"Layer wise Relevance Propagation","text":"<p>Layer-wise Relevance Propagation (LRP) is an interpretability technique used specifically for neural networks to understand the contribution of individual features or neurons in a neural model towards its predictions.</p> <p>The idea behind LRP is to assign relevance scores to each neuron in the network, indicating how much it contributes to the final prediction. These relevance scores are then backpropagated through the layers of the network, distributing the relevance from output layer back to input layer.</p>"},{"location":"Layer-wise%20Relevance%20Propagation/#general-steps-for-lrp","title":"General Steps for LRP","text":"<ol> <li>Initialize relevance: Start by assigning relevance scores to the neurons in the output layer based on the model's prediction. The relevance is typically higher for neurons that contribute more to the prediction.</li> <li>Relevance propagation: Starting from the output layer, propagate the relevance backward through the network. This involves distributing the relevance from each neuron to its input neurons in the previous layer based on their contribution.</li> <li>Relevance redistribution: At each layer, the relevance is redistributed among the input neurons based on their connection strengths and activation patterns. The redistribution rule can be designed to follow specific guidelines, such as conservation properties or heuristics, to allocate relevance appropriately.</li> <li>Continue propagation: Repeat the relevance propagation and redistribution steps layer by layer until reaching the input layer. This process attributes relevance scores to the input features, indicating their importance in the final prediction.</li> </ol>"},{"location":"What%20If%20Tool/","title":"What If Tool","text":""},{"location":"What%20If%20Tool/#what-is-the-wit","title":"What is the WIT?","text":"<p>Developed by Google, What If Tool is an open source tool to probe and understand ML models. It was used to understand the relationship between the independent variables and the outcomes. Due to it being easy to use and easy to generate visualizations with, it is widely used for explainability.</p> <p>We can use WIT in three formats: - Regular Jupyter Notebook - Google Colab Notebook - Tensorboard based Visualizations</p>"},{"location":"What%20If%20Tool/#checking-for-bias-in-model-using-wit","title":"Checking for Bias in Model using WIT","text":"<p>The What If Tool is very useful for checking whether our ML Model contains any biases regarding a particular feature such as gender, race, age, etc.</p> <ol> <li> <p>Prepare your data and Train your Model: Prepare your data and ensure that it is properly cleaned and labeled. Train your model on the dataset and save it.</p> </li> <li> <p>Loading into WIT: Load your model into WIT interface by importing it into your python environment and connecting it to the WIT tool</p> </li> <li> <p>Define Sensitive Features: Identify features in your data that may lead to biases. It may be related to gender, race, age or any other sensitive attribute.</p> </li> <li> <p>Explore data using WIT: Using WIT's interface, we can explore and interact with the data instances. We can filter, sort and group the instances based on features.</p> </li> <li> <p>Analyze model behavior: Use WIT to evaluate the models predictions and observe how they vary across various subgroups defined by the sensitive features. Look for patterns that indicate potential biases in the model's predictions/decisions. An example might be some groups getting consistently higher or lower predictions than others.</p> </li> <li> <p>Mitigate Biases: If you identify biases, you can use WIT to experiment with various mitigations strategies. You can try manipulating the input parameters to check how the models predictions change or apply fairness techniques like reweighting or bias-correction methods</p> </li> </ol>"},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p>"}]}